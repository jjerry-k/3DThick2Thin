{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, tqdm, time, random\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from tensorflow.keras import models, layers, losses, optimizers\n",
    "\n",
    "from loss import *\n",
    "from utils import *\n",
    "from metric import *\n",
    "from network import *\n",
    "\n",
    "random.seed(777)\n",
    "tf.set_random_seed(777)\n",
    "np.random.seed(777)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================\n",
    "# ======================================== Gil ===========================================\n",
    "# ============================================================================================\n",
    "\n",
    "# root_dir = '../data/for_section/'\n",
    "\n",
    "# fold_idx = 1\n",
    "\n",
    "# train_path = os.path.join(root_dir, 'for_fold_%d'%fold_idx)\n",
    "\n",
    "# val_path = os.path.join(root_dir, 'for_fold_%d_val'%fold_idx)\n",
    "\n",
    "# total = data_loader(train_path)\n",
    "\n",
    "# data = []\n",
    "# label = []\n",
    "# n_size = 32\n",
    "# n_slice = 16\n",
    "# cs_strides = 16\n",
    "# a_strides = 2\n",
    "\n",
    "# #print(test.shape)\n",
    "\n",
    "# for i in range(1):\n",
    "#     test = total[i]\n",
    "#     cor, sag, axi = test.shape\n",
    "#     means = [test[idx:idx+n_size, jdx:jdx+n_size, kdx:kdx+(n_slice*6)].mean() \n",
    "#              for idx in range(0, cor-n_size, cs_strides) \n",
    "#              for jdx in range(0, sag-n_size, cs_strides) \n",
    "#              for kdx in range(0, axi-(n_slice*6), a_strides)]\n",
    "\n",
    "\n",
    "#     for idx in tqdm.tqdm_notebook(range(0, cor-n_size, cs_strides)):\n",
    "#         for jdx in range(0, sag-n_size, cs_strides):\n",
    "#             for kdx in range(0, axi-(n_slice*6), a_strides):\n",
    "#                 tmp = test[idx:idx+n_size, jdx:jdx+n_size, kdx:kdx+(n_slice*6)]\n",
    "#                 if tmp.mean() > np.mean(means)+10:\n",
    "#                     label.append(tmp)\n",
    "#                     tmp = np.array(np.dsplit(tmp, n_slice))\n",
    "#                     tmp = tmp.mean(axis=-1)\n",
    "#                     tmp = np.transpose(tmp, [1,2,0])\n",
    "#                     data.append(tmp)\n",
    "                \n",
    "# label = np.array(label)[..., np.newaxis]\n",
    "# data = np.array(data)[..., np.newaxis]\n",
    "# print(data.shape)\n",
    "# print(label.shape)\n",
    "\n",
    "\n",
    "# # Prepare Validation\n",
    "# val_data = []\n",
    "# val_label = []\n",
    "\n",
    "# # Prepare Validation\n",
    "# scan_list = sorted(os.listdir(val_path))[1:3]\n",
    "# for scan in scan_list:\n",
    "#     dante_path = os.path.join(val_path, scan, 'T1SPACE09mmISOPOSTwDANTE')\n",
    "#     img_name = [i for i in os.listdir(dante_path) if '.nii' in i and '_rsl' not in i][0]\n",
    "#     #print(img_name)\n",
    "#     val = nib.load(os.path.join(dante_path, img_name))\n",
    "#     val = check_data(val.get_data())\n",
    "\n",
    "# cor, sag, axi = test.shape\n",
    "# #print(test.shape)\n",
    "# for idx in tqdm.tqdm_notebook(range(0, cor-n_size, cs_strides)):\n",
    "#     for jdx in range(0, sag-n_size, cs_strides):\n",
    "#         for kdx in range(0, axi-(n_slice*6), a_strides):\n",
    "#             tmp = val[idx:idx+n_size, jdx:jdx+n_size, kdx:kdx+(n_slice*6)]\n",
    "# #             means.append(tmp.mean())\n",
    "#             if tmp.mean() > 225:\n",
    "#                 val_label.append(tmp)\n",
    "#                 tmp = np.array(np.dsplit(tmp, n_slice))\n",
    "#                 tmp = tmp.mean(axis=-1)\n",
    "#                 tmp = np.transpose(tmp, [1,2,0])\n",
    "#                 val_data.append(tmp)\n",
    "                \n",
    "# val_label = np.array(val_label)[..., np.newaxis]\n",
    "# val_data = np.array(val_data)[..., np.newaxis]\n",
    "# print(val_data.shape)\n",
    "# print(val_label.shape)\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================================\n",
    "# ======================================== CC 359 ===========================================\n",
    "# ============================================================================================\n",
    "\n",
    "\n",
    "# data_root = '../data/Original/'\n",
    "# data_lists = sorted(os.listdir(data_root))\n",
    "# ph15_lists = [i for i in data_lists if '_philips_15' in i]\n",
    "# ph3_lists = [i for i in data_lists if '_philips_3' in i]\n",
    "# si15_lists = [i for i in data_lists if '_siemens_15' in i]\n",
    "# si3_lists = [i for i in data_lists if '_siemens_3' in i]\n",
    "# ge15_lists = [i for i in data_lists if '_ge_15' in i]\n",
    "# ge3_lists = [i for i in data_lists if '_ge_3' in i]\n",
    "\n",
    "# train_list = ['CC0001', 'CC0060', 'CC0120', 'CC0180', 'CC0241', 'CC0300']\n",
    "\n",
    "# val_list = ['CC0002', 'CC0061', 'CC0121', 'CC0181', 'CC0242', 'CC0301']\n",
    "\n",
    "# print(len(data_lists))\n",
    "# print(len(ph15_lists))\n",
    "# print(len(ph3_lists))\n",
    "# print(len(ge15_lists))\n",
    "# print(len(ge3_lists))\n",
    "# print(len(si15_lists))\n",
    "# print(len(si3_lists))\n",
    "\n",
    "# train_img = {name : nib.load(os.path.join(data_root, name)).get_data().astype(np.uint16) \n",
    "#              for name in data_lists if name.split('_')[0] in train_list}\n",
    "\n",
    "\n",
    "# data = []\n",
    "# label = []\n",
    "# n_size = 32\n",
    "# n_slice = 16\n",
    "# cs_strides = 16\n",
    "# a_strides = 2\n",
    "\n",
    "# #print(test.shape)\n",
    "\n",
    "# for i in train_img:\n",
    "#     test = train_img[i]\n",
    "#     cor, sag, axi = test.shape\n",
    "#     means = [test[idx:idx+n_size, jdx:jdx+n_size, kdx:kdx+(n_slice*6)].mean() \n",
    "#              for idx in range(0, cor-n_size, cs_strides) \n",
    "#              for jdx in range(0, sag-n_size, cs_strides) \n",
    "#              for kdx in range(0, axi-(n_slice*6), a_strides)]\n",
    "#     print(np.mean(means))\n",
    "#     for idx in tqdm.tqdm_notebook(range(0, cor-n_size, cs_strides)):\n",
    "#         for jdx in range(0, sag-n_size, cs_strides):\n",
    "#             for kdx in range(0, axi-(n_slice*6), a_strides):\n",
    "#                 tmp = test[idx:idx+n_size, jdx:jdx+n_size, kdx:kdx+(n_slice*6)]\n",
    "#                 if tmp.mean() > np.mean(means)+10:\n",
    "#                     label.append(tmp)\n",
    "#                     tmp = np.array(np.dsplit(tmp, n_slice))\n",
    "#                     tmp = tmp.mean(axis=-1)\n",
    "#                     tmp = np.transpose(tmp, [1,2,0])\n",
    "#                     data.append(tmp)\n",
    "                \n",
    "# label = np.array(label)[..., np.newaxis]\n",
    "# data = np.array(data)[..., np.newaxis]\n",
    "# print(data.shape)\n",
    "# print(label.shape)\n",
    "\n",
    "# val_img = {name : nib.load(os.path.join(data_root, name)).get_data().astype(np.uint16) \n",
    "#              for name in data_lists if name.split('_')[0] in val_list}\n",
    "\n",
    "\n",
    "# val_data = []\n",
    "# val_label = []\n",
    "# cs_strides = 16\n",
    "# a_strides = 8\n",
    "# #print(test.shape)\n",
    "\n",
    "# for i in val_img:\n",
    "#     test = val_img[i]\n",
    "#     cor, sag, axi = test.shape\n",
    "#     means = [test[idx:idx+n_size, jdx:jdx+n_size, kdx:kdx+(n_slice*6)].mean() \n",
    "#              for idx in range(0, cor-n_size, cs_strides) \n",
    "#              for jdx in range(0, sag-n_size, cs_strides) \n",
    "#              for kdx in range(0, axi-(n_slice*6), a_strides)]\n",
    "#     print(np.mean(means))\n",
    "#     for idx in tqdm.tqdm_notebook(range(0, cor-n_size, cs_strides)):\n",
    "#         for jdx in range(0, sag-n_size, cs_strides):\n",
    "#             for kdx in range(0, axi-(n_slice*6), a_strides):\n",
    "#                 tmp = test[idx:idx+n_size, jdx:jdx+n_size, kdx:kdx+(n_slice*6)]\n",
    "#                 if tmp.mean() > np.mean(means)+10:\n",
    "#                     val_label.append(tmp)\n",
    "#                     tmp = np.array(np.dsplit(tmp, n_slice))\n",
    "#                     tmp = tmp.mean(axis=-1)\n",
    "#                     tmp = np.transpose(tmp, [1,2,0])\n",
    "#                     val_data.append(tmp)\n",
    "                \n",
    "# val_label = np.array(val_label)[..., np.newaxis]\n",
    "# val_data = np.array(val_data)[..., np.newaxis]\n",
    "# print(val_data.shape)\n",
    "# print(val_label.shape)\n",
    "\n",
    "\n",
    "# ============================================================================================\n",
    "# ======================================== Sector ===========================================\n",
    "# ============================================================================================\n",
    "\n",
    "\n",
    "root_dir = '../data/for_section/'\n",
    "\n",
    "fold_idx = 1\n",
    "\n",
    "train_path = os.path.join(root_dir, 'for_fold_%d'%fold_idx)\n",
    "\n",
    "val_path = os.path.join(root_dir, 'for_fold_%d_val'%fold_idx)\n",
    "\n",
    "total = data_loader(train_path)\n",
    "\n",
    "data = []\n",
    "label = []\n",
    "n_size = 32\n",
    "n_slice = 16\n",
    "cs_strides = 8\n",
    "a_strides = 1\n",
    "\n",
    "#print(test.shape)\n",
    "\n",
    "for i in range(1):\n",
    "    test = total[i][64:192, 64:192, :97]\n",
    "    cor, sag, axi = test.shape\n",
    "    print(cor, sag, axi)\n",
    "    print(range(0, cor-n_size, cs_strides))\n",
    "    print(range(0, sag-n_size, cs_strides))\n",
    "    print(range(0, axi-(n_slice*6), a_strides))\n",
    "    means = [test[idx:idx+n_size, jdx:jdx+n_size, kdx:kdx+(n_slice*6)].mean() \n",
    "             for idx in range(0, cor-n_size, cs_strides) \n",
    "             for jdx in range(0, sag-n_size, cs_strides) \n",
    "             for kdx in range(0, axi-(n_slice*6), a_strides)]\n",
    "    \n",
    "    for idx in tqdm.tqdm_notebook(range(0, cor-n_size, cs_strides)):\n",
    "        for jdx in range(0, sag-n_size, cs_strides):\n",
    "            for kdx in range(0, axi-(n_slice*6), a_strides):\n",
    "                tmp = test[idx:idx+n_size, jdx:jdx+n_size, kdx:kdx+(n_slice*6)]\n",
    "                if tmp.mean() > np.mean(means)+10:\n",
    "                    label.append(tmp)\n",
    "                    tmp = np.array(np.dsplit(tmp, n_slice))\n",
    "                    tmp = tmp.mean(axis=-1)\n",
    "                    tmp = np.transpose(tmp, [1,2,0])\n",
    "                    data.append(tmp)\n",
    "                \n",
    "label = np.array(label)[..., np.newaxis]\n",
    "data = np.array(data)[..., np.newaxis]\n",
    "print(data.shape)\n",
    "print(label.shape)    \n",
    "\n",
    "# Prepare Validation\n",
    "val_data = []\n",
    "val_label = []\n",
    "\n",
    "# Prepare Validation\n",
    "scan_list = sorted(os.listdir(val_path))[1:2]\n",
    "for scan in scan_list:\n",
    "    dante_path = os.path.join(val_path, scan, 'T1SPACE09mmISOPOSTwDANTE')\n",
    "    img_name = [i for i in os.listdir(dante_path) if '.nii' in i and '_rsl' not in i][0]\n",
    "    #print(img_name)\n",
    "    val = nib.load(os.path.join(dante_path, img_name))\n",
    "    val = check_data(val.get_data())[64:192, 64:192, :97]\n",
    "\n",
    "cor, sag, axi = val.shape\n",
    "#print(test.shape)\n",
    "print(cor, sag, axi)\n",
    "print(range(0, cor-n_size, cs_strides))\n",
    "print(range(0, sag-n_size, cs_strides))\n",
    "print(range(0, axi-(n_slice*6), a_strides))\n",
    "\n",
    "means = [test[idx:idx+n_size, jdx:jdx+n_size, kdx:kdx+(n_slice*6)].mean() \n",
    "             for idx in range(0, cor-n_size, cs_strides) \n",
    "             for jdx in range(0, sag-n_size, cs_strides) \n",
    "             for kdx in range(0, axi-(n_slice*6), a_strides)]\n",
    "\n",
    "for idx in tqdm.tqdm_notebook(range(0, cor-n_size, cs_strides)):\n",
    "    for jdx in range(0, sag-n_size, cs_strides):\n",
    "        for kdx in range(0, axi-(n_slice*6), a_strides):\n",
    "            tmp = val[idx:idx+n_size, jdx:jdx+n_size, kdx:kdx+(n_slice*6)]\n",
    "#             means.append(tmp.mean())\n",
    "            if tmp.mean() > np.mean(means)+10:\n",
    "                val_label.append(tmp)\n",
    "                tmp = np.array(np.dsplit(tmp, n_slice))\n",
    "                tmp = tmp.mean(axis=-1)\n",
    "                tmp = np.transpose(tmp, [1,2,0])\n",
    "                val_data.append(tmp)\n",
    "                \n",
    "val_label = np.array(val_label)[..., np.newaxis]\n",
    "val_data = np.array(val_data)[..., np.newaxis]\n",
    "print(val_data.shape)\n",
    "print(val_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_channels = 64\n",
    "G = SR3D_res(residual_channel=residual_channels, \n",
    "             layer_activation='leaky_relu', \n",
    "             name='3D_SR_Gen')\n",
    "D = dis3D_res(residual_channel=residual_channels, \n",
    "              name='3D_SR_Dis')\n",
    "\n",
    "D.compile(optimizer=optimizers.Adam(lr=0.0001, epsilon=1e-8), loss=losses.binary_crossentropy)\n",
    "\n",
    "D.trainable=False\n",
    "\n",
    "A = models.Model(inputs=G.input, outputs = [G.output, D(G.input)], name='GAN')\n",
    "A.compile(optimizer=optimizers.Adam(lr=0.0001, epsilon=1e-8), loss=[mse_grad_loss, losses.binary_crossentropy], \n",
    "          loss_weights=[10, 1], metrics={'3D_SR_Gen_output_act':[mutual_information, gradient_3d_loss, psnr]})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = time.ctime().split(' ')\n",
    "\n",
    "ckpt_root = './checkpoint/%s_%02d_%s/RBSRGAN%d_msegrad_GIL_sector'%(date[1], int(date[3]), date[-1], residual_channels)\n",
    "result_root = './result/%s_%02d_%s/RBSRGAN%d_msegrad_GIL_sector'%(date[1], int(date[3]), date[-1], residual_channels)\n",
    "print(ckpt_root_root)\n",
    "print(result_root)\n",
    "try:\n",
    "    os.makedirs(ckpt_root)\n",
    "    print(\"\\nMake Save Directory!\\n\")\n",
    "except:\n",
    "    print(\"\\nDirectory Already Exist!\\n\")\n",
    "\n",
    "try:\n",
    "    os.makedirs(result_root)\n",
    "    print(\"\\nMake Save Directory!\\n\")\n",
    "except:\n",
    "    print(\"\\nDirectory Already Exist!\\n\")\n",
    "    \n",
    "\n",
    "model_json = A.to_json()\n",
    "with open(os.path.join(ckpt_root, \"model.json\"), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "print(\"\\nModel Saved!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=100\n",
    "batch_size = 8\n",
    "train_length = len(data)\n",
    "val_length = len(val_data)\n",
    "num_iter = int(np.ceil(train_length/batch_size))\n",
    "num_val_iter = int(np.ceil(val_length/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = {\"Generator_Total\" : [], \"Generator_Style\" : [], \"Generator_AD\" : [], \"mi\": [], \"mse\": [], \"grad\": [], \"psnr\": [], \"Discriminator_AD\" : []}\n",
    "val_loss = {\"Generator_Total\" : [], \"Generator_Style\" : [], \"Generator_AD\" : [], \"mi\": [], \"mse\": [], \"grad\": [], \"psnr\": []}\n",
    "top_gen_loss = float('inf')\n",
    "stop_patience = 5\n",
    "stop_cnt = 0\n",
    "save_patience = 2\n",
    "save_cnt = 0\n",
    "top_epoch = 0\n",
    "prev_val_loss = 0\n",
    "prev_val_mi = 0\n",
    "prev_val_grad = 0\n",
    "prev_val_psnr = 0\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_t_g_total = 0\n",
    "    epoch_t_g_style = 0\n",
    "    epoch_t_g_dis = 0\n",
    "    epoch_t_d_dis = 0\n",
    "    epoch_t_mse = 0\n",
    "    epoch_t_mi = 0\n",
    "    epoch_t_grad = 0\n",
    "    epoch_t_psnr = 0\n",
    "    \n",
    "    shuffle_idx = np.random.choice(train_length, train_length, replace=False)\n",
    "    for i, step in enumerate(range(0, train_length, batch_size)):\n",
    "\n",
    "        #print(step, shuffle_idx[step:step+batch_size])\n",
    "\n",
    "        # Generate fake images\n",
    "        step_idx = shuffle_idx[step:step+batch_size]\n",
    "        fake_imgs = G.predict(data[step_idx])\n",
    "\n",
    "        # Train Discriminator\n",
    "        dis_input = np.concatenate([fake_imgs, label[step_idx]])\n",
    "        dis_label =np.concatenate([np.zeros((len(step_idx), 1)),\n",
    "                                 np.ones((len(step_idx), 1))])\n",
    "        Dis_Loss = D.train_on_batch(dis_input, dis_label)\n",
    "\n",
    "        # Train Generator\n",
    "        Gan_Loss = A.train_on_batch(data[step_idx], [label[step_idx], np.ones((len(step_idx), 1))])\n",
    "        \n",
    "        epoch_t_g_total += Gan_Loss[0]\n",
    "        epoch_t_g_style += Gan_Loss[-5]\n",
    "        epoch_t_g_dis += Gan_Loss[-4]\n",
    "        epoch_t_mse += np.mean(np.square(fake_imgs-label[step_idx]))\n",
    "        epoch_t_mi += Gan_Loss[-3]\n",
    "        epoch_t_grad += Gan_Loss[-2]\n",
    "        epoch_t_psnr += Gan_Loss[-1]\n",
    "        epoch_t_d_dis += Dis_Loss\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        plt.plot(train_loss[\"Generator_Style\"], '.-')    \n",
    "        plt.plot(val_loss[\"Generator_Style\"], '.-')\n",
    "        #plt.suptitle('Epoch : {}/{}, Step : {}/{}'.format())\n",
    "        plt.title(\"Epoch : %d/%d, Step : %d/%d\\nGen Loss : %.5f, Val gen loss : %.5f\\n\\\n",
    "                    Gen MI : %.5f, Val MI : %.5f\\nGen Grad : %.5f, Val Grad : %.5f\\n\\\n",
    "                    Gen PSNR : %.5f, Val PSNR : %.5f\"\n",
    "                  %(epoch, epochs, step, train_length, \n",
    "                    epoch_t_g_style/(i+1), prev_val_loss, \n",
    "                    epoch_t_mi/(i+1), prev_val_mi, \n",
    "                    epoch_t_grad/(i+1), prev_val_grad, \n",
    "                    epoch_t_psnr/(i+1), prev_val_psnr))\n",
    "        plt.legend(['train', 'validation'], loc='upper right')\n",
    "        plt.show()\n",
    "    \n",
    "    train_loss[\"Generator_Total\"].append(epoch_t_g_total/num_iter)\n",
    "    train_loss[\"Generator_Style\"].append(epoch_t_g_style/num_iter)\n",
    "    train_loss[\"Generator_AD\"].append(epoch_t_g_dis/num_iter)\n",
    "    train_loss[\"Discriminator_AD\"].append(epoch_t_d_dis/num_iter)\n",
    "    train_loss[\"mse\"].append(epoch_t_mse/num_iter)\n",
    "    train_loss[\"mi\"].append(epoch_t_mi/num_iter)\n",
    "    train_loss[\"grad\"].append(epoch_t_grad/num_iter)\n",
    "    train_loss[\"psnr\"].append(epoch_t_psnr/num_iter)\n",
    "    \n",
    "    \n",
    "    epoch_v_g_total = 0\n",
    "    epoch_v_g_style = 0\n",
    "    epoch_v_g_dis = 0\n",
    "    epoch_v_mse = 0\n",
    "    epoch_v_mi = 0\n",
    "    epoch_v_grad = 0\n",
    "    epoch_v_psnr = 0\n",
    "    \n",
    "    for j, val_idx in enumerate(range(0, val_length, batch_size)):\n",
    "        #val_idx = j\n",
    "        val_y2 = np.ones([len(val_data[val_idx:val_idx+batch_size]), 1])\n",
    "        #print(j, val_idx, len(val_data[val_idx:val_idx+batch_size]))\n",
    "        V_loss = A.test_on_batch(val_data[val_idx:val_idx+batch_size], \n",
    "                                           [val_label[val_idx:val_idx+batch_size], val_y2])\n",
    "        V_output, _= A.predict(val_data[val_idx:val_idx+batch_size])\n",
    "        #print(\"Validation Inference\")\n",
    "        epoch_v_g_total += V_loss[0]\n",
    "        epoch_v_g_style += V_loss[-5]\n",
    "        epoch_v_g_dis += V_loss[-4]\n",
    "        epoch_v_mse += np.mean(np.square(V_output-val_label[val_idx:val_idx+batch_size]))\n",
    "        epoch_v_mi += V_loss[-3]\n",
    "        epoch_v_grad += V_loss[-2]\n",
    "        epoch_v_psnr += V_loss[-1]\n",
    "    \n",
    "    val_loss[\"Generator_Total\"].append(epoch_v_g_total/num_val_iter)\n",
    "    val_loss[\"Generator_Style\"].append(epoch_v_g_style/num_val_iter)\n",
    "    val_loss[\"Generator_AD\"].append(epoch_v_g_dis/num_val_iter)\n",
    "    val_loss[\"mse\"].append(epoch_v_mse/num_val_iter)\n",
    "    val_loss[\"mi\"].append(epoch_v_mi/num_val_iter)\n",
    "    val_loss[\"grad\"].append(epoch_v_grad/num_val_iter)\n",
    "    val_loss[\"psnr\"].append(epoch_v_psnr/num_val_iter)\n",
    "    \n",
    "    mean_val_loss = epoch_v_g_style/num_val_iter\n",
    "    prev_val_loss = mean_val_loss\n",
    "    prev_val_mi = epoch_v_mi/num_val_iter\n",
    "    prev_val_grad = epoch_v_grad/num_val_iter\n",
    "    prev_val_psnr = epoch_v_psnr/num_val_iter\n",
    "    \n",
    "    # Saving Phase\n",
    "    if mean_val_loss < top_gen_loss:\n",
    "        stop_cnt = 0\n",
    "        top_gen_loss = mean_val_loss\n",
    "        if epoch == 0: \n",
    "            A.save_weights(os.path.join(ckpt_root, \"%05d_%.4f_%.4f.h5\"%(epoch+1, epoch_t_g_style/num_iter, top_gen_loss)))\n",
    "            top_epoch = epoch\n",
    "        elif top_epoch + save_patience > epoch : pass\n",
    "        else:\n",
    "            A.save_weights(os.path.join(ckpt_root, \"%05d_%.4f_%.4f.h5\"%(epoch+1, epoch_t_g_style/num_iter, top_gen_loss)))\n",
    "            top_epoch = epoch\n",
    "    else:\n",
    "        stop_cnt+=1\n",
    "    \n",
    "    if stop_cnt == stop_patience : break\n",
    "\n",
    "print(\"Saved Model with %.5f!\"%top_gen_loss)\n",
    "\n",
    "train_df = pd.DataFrame(train_loss)\n",
    "train_df.to_csv(os.path.join(result_root,'train_loss.csv'))\n",
    "\n",
    "val_df = pd.DataFrame(val_loss)\n",
    "val_df.to_csv(os.path.join(result_root, 'val_loss.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For CC359\n",
    "\n",
    "test_list = ['CC0003', 'CC0062', 'CC0122', 'CC0182', 'CC0243', 'CC0302']\n",
    "test_img = {name : nib.load(os.path.join(data_root, name)).get_data().astype(np.uint16)\n",
    "             for name in data_lists if name.split('_')[0] in test_list}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test in test_img:\n",
    "    print(test_img[test].shape)\n",
    "    resize_for_crop_infer(test_img[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_data(test_img[test]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_for_crop_infer(img):\n",
    "    cor, sag, axi = img.shape\n",
    "    c_pad = cor%8\n",
    "    s_pad = sag%8\n",
    "    \n",
    "    new_cor = cor + c_pad\n",
    "    new_sag = sag + s_pad\n",
    "    \n",
    "    \n",
    "    h = int(new_cor*0.625)\n",
    "    h_cut = int(new_cor*0.125)\n",
    "    h_margin = int(new_cor*0.375)\n",
    "    \n",
    "    w = int(new_sag*0.625)\n",
    "    w_cut = int(new_sag*0.125)\n",
    "    w_margin = int(new_sag*0.375)\n",
    "    \n",
    "    slice_dict = {\n",
    "    1:[[0, h], [0, w], [0, h-h_cut], [0, w-w_cut]], \n",
    "    2:[[0, h], [w_margin, w+w_margin], [0, h-h_cut], [w_cut, new_sag]], \n",
    "    3:[[h_margin, new_cor], [0, w], [h_cut, new_cor], [0, w-w_cut]],\n",
    "    4:[[h_margin, new_cor], [w_margin, new_sag], [h_cut, new_cor], [w_cut, new_sag]]\n",
    "    }\n",
    "    \n",
    "    print(new_cor, h, h_margin, h_cut)\n",
    "    print(new_sag, w, w_margin, w_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_for_crop_infer(test_img[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 176\n",
    "print(size*0.625)\n",
    "\n",
    "print(size*0.125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Gil\n",
    "scan_list = sorted(os.listdir(val_path))[1:]\n",
    "for scan in scan_list:\n",
    "    dante_path = os.path.join(val_path, scan, 'T1SPACE09mmISOPOSTwDANTE')\n",
    "    img_name = [i for i in os.listdir(dante_path) if '.nii' in i and '_rsl' not in i][0]\n",
    "    #print(img_name)\n",
    "    val = nib.load(os.path.join(dante_path, img_name))\n",
    "    val = check_data(val.get_data())\n",
    "    \n",
    "test_in = []\n",
    "cor, sag, axi = val.shape\n",
    "tmp = np.array(np.dsplit(val, axi//6))\n",
    "tmp = tmp.mean(axis=-1)\n",
    "tmp = np.transpose(tmp, [1, 2, 0])\n",
    "\n",
    "#half_top = int(np.floor(tmp.shape[-1]/2))\n",
    "#half_bot = int(np.ceil(tmp.shape[-1]/2))\n",
    "\n",
    "recon = np.zeros(shape=[cor, sag, axi])\n",
    "\n",
    "slice_dict = {\n",
    "    1:[[0, 160], [0, 160], [0, 128], [0, 128]], \n",
    "    2:[[0, 160], [96, 256], [0, 128], [32, 256]], \n",
    "    3:[[96, 256], [0, 160], [32, 256], [0, 128]],\n",
    "    4:[[96, 256], [96, 256], [32, 256], [32, 256]]\n",
    "}\n",
    "test = {}\n",
    "sli = 12\n",
    "i = 0\n",
    "for row in range(2):\n",
    "    row_start = row*128\n",
    "    for col in range(2):\n",
    "        \n",
    "        col_start = col*128\n",
    "        print(i, row_start, col_start)\n",
    "        test[i] = G.predict(tmp[np.newaxis, \n",
    "                                   slice_dict[i+1][0][0]:slice_dict[i+1][0][1],\n",
    "                                   slice_dict[i+1][1][0]:slice_dict[i+1][1][1],\n",
    "                                   :, np.newaxis])\n",
    "        recon[row_start:row_start+128, \n",
    "              col_start:col_start+128] = test[i][0,\n",
    "                                                  slice_dict[i+1][2][0]:slice_dict[i+1][2][1],\n",
    "                                                  slice_dict[i+1][3][0]:slice_dict[i+1][3][1], :, 0]\n",
    "        i += 1\n",
    "# test_3d_2 = net.predict(tmp[np.newaxis, ..., half_top:, np.newaxis])\n",
    "\n",
    "aff = np.eye(4)\n",
    "aff[2, 2]=6\n",
    "\n",
    "nib.save(nib.Nifti1Image(tmp, aff), os.path.join(result_root, 'val_input.nii'))\n",
    "nib.save(nib.Nifti1Image(recon, np.eye(4)), os.path.join(result_root, 'val_pred.nii'))\n",
    "nib.save(nib.Nifti1Image(val, np.eye(4)), os.path.join(result_root, 'val_label.nii'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_vox = G.predict(tmp[np.newaxis, 64:192, 64:192, :16 , np.newaxis])[0,..., 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "recon = recon.astype(np.float32)\n",
    "whole_mse = np.mean(np.square(recon[64:192, 64:192, :96]-val[64:192, 64:192, :96]))\n",
    "vox_mse = np.mean(np.square(recon_vox-val[64:192, 64:192, :96]))\n",
    "\n",
    "whole_mae = np.mean(np.abs(recon[64:192, 64:192, :96]-val[64:192, 64:192, :96]))\n",
    "vox_mae = np.mean(np.abs(recon_vox-val[64:192, 64:192, :96]))\n",
    "\n",
    "whole_psnr = sess.run(psnr(recon[64:192, 64:192, :96], val[64:192, 64:192, :96]))\n",
    "vox_psnr = sess.run(psnr(recon_vox, val[64:192, 64:192, :96]))\n",
    "\n",
    "ref = sess.run(mutual_information(val[np.newaxis, 64:192, 64:192, :96, np.newaxis], \n",
    "                                  val[np.newaxis, 64:192, 64:192, :96, np.newaxis]))[0]\n",
    "whole_mi = sess.run(mutual_information(recon[np.newaxis, 64:192, 64:192, :96, np.newaxis], \n",
    "                                        val[np.newaxis, 64:192, 64:192, :96, np.newaxis]))[0]\n",
    "vox_mi = sess.run(mutual_information(recon_vox[np.newaxis, ..., np.newaxis], \n",
    "                                      val[np.newaxis, 64:192, 64:192, :96, np.newaxis]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sqrt(whole_mse), np.sqrt(vox_mse))\n",
    "print(whole_mae, vox_mae)\n",
    "print(whole_psnr, vox_psnr)\n",
    "print(whole_mi/ref, vox_mi/ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aff = np.eye(4)\n",
    "aff[2, 2]=6\n",
    "\n",
    "nib.save(nib.Nifti1Image(recon_vox, np.eye(4)), os.path.join(result_root, 'sector.nii'))\n",
    "nib.save(nib.Nifti1Image(recon[64:192, 64:192, :96], np.eye(4)), os.path.join(result_root, 'whold.nii'))\n",
    "nib.save(nib.Nifti1Image(val[64:192, 64:192, :96], np.eye(4)), os.path.join(result_root, 'label.nii'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_thick = np.array(np.dsplit(total[0], total[0].shape[-1]//6))\n",
    "train_thick = train_thick.mean(axis=-1)\n",
    "train_thick = np.transpose(train_thick, [1, 2, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor, sag, axi = total[0].shape\n",
    "\n",
    "recon = np.zeros(shape=[cor, sag, axi])\n",
    "\n",
    "slice_dict = {\n",
    "    1:[[0, 160], [0, 160], [0, 128], [0, 128]], \n",
    "    2:[[0, 160], [96, 256], [0, 128], [32, 256]], \n",
    "    3:[[96, 256], [0, 160], [32, 256], [0, 128]],\n",
    "    4:[[96, 256], [96, 256], [32, 256], [32, 256]]\n",
    "}\n",
    "test = {}\n",
    "sli = 12\n",
    "i = 0\n",
    "for row in range(2):\n",
    "    row_start = row*128\n",
    "    for col in range(2):\n",
    "        \n",
    "        col_start = col*128\n",
    "        print(i, row_start, col_start)\n",
    "        test[i] = G.predict(train_thick[np.newaxis, \n",
    "                                   slice_dict[i+1][0][0]:slice_dict[i+1][0][1],\n",
    "                                   slice_dict[i+1][1][0]:slice_dict[i+1][1][1],\n",
    "                                   :, np.newaxis])\n",
    "        recon[row_start:row_start+128, \n",
    "              col_start:col_start+128] = test[i][0,\n",
    "                                                  slice_dict[i+1][2][0]:slice_dict[i+1][2][1],\n",
    "                                                  slice_dict[i+1][3][0]:slice_dict[i+1][3][1], :, 0]\n",
    "        i += 1\n",
    "\n",
    "        \n",
    "aff = np.eye(4)\n",
    "aff[2, 2]=6\n",
    "nib.save(nib.Nifti1Image(train_thick, aff), os.path.join(result_root, 'train_input.nii'))\n",
    "nib.save(nib.Nifti1Image(recon, np.eye(4)), os.path.join(result_root, 'train_pred.nii'))\n",
    "nib.save(nib.Nifti1Image(total[0], np.eye(4)), os.path.join(result_root, 'train_label.nii'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
